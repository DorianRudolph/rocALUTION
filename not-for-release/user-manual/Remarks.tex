\chapter{Remarks}

\section{Performance}
\label{remark-performance}

\begin{itemize}
\itemsep0em

\item Solvers -- typically the PARALUTION solvers perform better than MKL/CUSPARSE based solvers due to faster preconditioners and better routines for matrix and vector operations.

\item Solvers -- you can also build the solvers (via Build()) on the accelerator. In many cases this is faster than computing it on the host, especially for GPU backends.

\item Sizes -- small-sized problems tend to perform better on the host (CPU) due to the good caching system, while large-sized problems typically perform better on GPU devices.

\item Vectors -- avoid accessing via \emph{[]} operators, use techniques based on \emph{SetDataPtr()} and \emph{LeaveDataPtr()} instead.

\item Host/Threads -- by default the host OpenMP backend picks the maximum number of threads available. However, if your CPU supports HyperThreading, it will allow to run two times more threads than number of cores. This, in many cases, leads to lower performance. If your system supports HyperThreading, you may observe a performance increase by setting the number of threads (via the \emph{set\_omp\_threads\_paralution} function) equal to the number of physical cores.

\item Solving a system with multiple right-hand-sides -- if you need to solve a linear system multiple times, avoid constructing the solver/preconditioner every time.

\item Solving similar systems -- if you are solving similar linear systems, you might want to consider to use the same preconditioner to avoid long building phases.

\item Matrix formats -- in most of the cases, the classical CSR format performs very similar to all other formats on the CPU. On accelerators with many-cores (like GPU), formats like DIA and ELL typically perform better. However, for general sparse matrices one could use HYB format to avoid large memory overhead (e.g. in DIA or ELL formats). The multi-colored preconditioners could be performed in ELL for most of the matrices.

\item Matrix formats - not all matrix conversions are performed on the device, the platform will give you a warning if the object need to be moved.

\item Integration -- if you are deploying the PARALUTION library into another software framework try to design your integration functions to avoid \emph{init\_paralution()} and \emph{stop\_paralution()} every time you call a solver in the library. 

\item Compilation -- be sure to compile the library with the correct optimization level (\emph{-O3}).

\item Info -- check if your solver is really performed on the accelerator by printing the matrix information (\emph{my\_matrix.Info()}) just before calling the \emph{ls.Solve} function.

\item Info -- check the configuration of the library for your hardware with \emph{info\_paralution()}.

\item Mixed-precision defect correction -- this technique is recommended for accelerators (e.g. GPUs) with partial or no double precision support. The stopping criteria for the inner solver has to be tuned well for good performance. 

\item Plug-ins -- all plug-ins perform an additional copying of the data to construct the matrix, solver, preconditioner, etc. This results in overhead when calling the PARALUTION solvers/schemes. To avoid this, adapt the plug-in to your application as much as possible.

\item Verbose level -- it is a good practice to use the verbose level 2 to obtain critical messages regarding the performance.

\item Xeon Phi -- the allocation of memory on the Xeon Phi is slow, try to avoid unnecessary data allocation whenever is possible.

\end{itemize}

\section{Accelerators}
\label{remark-accelerator}

\begin{itemize}
\itemsep0em

\item Old GPUs - PARALUTION requires NVIDIA GPU with minimum compute capability 2.0, if the GPU is not 2.0 the computation will fall back to the OpenMP host backend.

\item Avoid PCI-Express communication -- whenever possible avoid extra PCI communication (like copying data from/to the accelerator), check also the internal structure of the functions.   

\item GPU init time -- if you are running your computation on a NVIDIA GPU card where no X Windows is running (for Linux OS), you can minimize the initialization time of the GPUs by \emph{nvidia-smi -pm 1} which eliminates reloading the driver every time you launch your GPU program.

\item Pinned memory -- pinned memory allocation (page-locked) are used for all host memory allocations when using the CUDA backend. This provides faster transfers over the PCI-Express and allows asynchronous data movement. By default this option is disabled, to enable the pinned memory allocation uncomment \emph{\#define PARALUTION\_CUDA\_PINNED\_MEMORY} in file \emph{src/utils/allocate\_free.hpp}

\item Async transfers -- the asynchronous transfers are available only for the CUDA backend so far. If async transfers are called from other backends they will perform simple sync move or copy.

\item Xeon Phi -- the Intel MIC architecture could be used also via the OpenCL backend. However the performance is not so great due to the fact that most of the kernels are optimize for GPU devices.

\item Xeon Phi -- you can tune the number OpenCL parameters, after the execution of \emph{cmake} and before compiling the library with \emph{make} edit the OpenCL hardware parameters located in \emph{src/utils/HardwareParameters.hpp}.

\item OpenCL (x86 CPU) -- the sparse-matrix vector multiplication in COO format is hanging, we are working to fix that.

\item OpenCL -- after calling the cmake you can set the block size and the warp size by editing \emph{src/utils/HardwareParameters.hpp}. After that you just need to compile the library with make. Alternatively you can modify the \emph{src/utils/ocl\_check\_hw.cpp} file.

\end{itemize}

\section{Correctness}
\label{remark-correctness}

\begin{itemize}
\itemsep0em

\item Matrix -- if you are assembling or modifying your matrix, you can check your matrix in octave/MATLAB by just writing it into a matrix-market file and read it via \emph{mmread()} function \cite{mm-read}. You can also input a MATLAB/octave matrix in such way.

\item Solver tolerance -- be sure you are setting the correct relative and absolute tolerance values for your problem.

\item Solver stopping criteria -- check the computation of the relative stopping criteria if it is based on $\frac{|b-Ax^k|_{2}}{|b-Ax^0|_{2}}$ or $\frac{|b-Ax^k|_{2}}{|b|_{2}}$.

\item Ill-conditioned problems -- solving very ill-conditioned problems by iterative methods without a proper preconditioning technique might produce wrong results. The solver could stop by showing a low relative tolerance based on the residual but this might be wrong. 

\item Ill-conditioned problems -- building the Krylov subspace for many ill-conditioned problems could be a tricky task. To ensure orthogonality in the subspace you might want to perform double orthogonalization (i.e. re-orthogonalization) to avoid rounding errors.

\item I/O files -- if you read/write matrices/vectors from files, check the ASCII format of the values (e.g. $34.3434$ or $3.43434E+01$).

\end{itemize}


\section{Compilation}
\label{remark-compilation}

\begin{itemize}
\itemsep0em

\item OpenMP backend -- the OpenMP support is by default enabled. To disable it you need to specify \emph{-DSUPPORT\_OMP=OFF} in the cmake

\item CUDA 5.5 and gcc 4.8 -- these compiler versions are incompatible (the compilation will report error \emph{"kernel launches from templates are not allowed in system files"}). Please use lower \emph{gcc} version, you can push the \emph{nvcc} compiler to use it by setting a link in the default CUDA installation directory (\emph{/usr/local/cuda}) - e.g. by running under root \emph{ln -s /usr/bin/gcc-4.4 /usr/local/cuda/bin/gcc}. Or try the \emph{-ccbin} option.

\item CUDA backend -- be sure that the paths are correctly set (e.g. for Linux \emph{export LD\_LIBRARY\_PATH= \$LD\_LIBRARY\_PATH:/usr/local/cuda/lib64/} and \emph{export CPLUS\_INCLUDE\_PATH=\$CPLUS\_INCLUDE\_PATH: /usr/local/cuda/include/}). Then you can run cmake with \emph{make  -DSUPPORT\_OCL=OFF -DSUPPORT\_CUDA=ON ..}

\item OpenCL backend -- similar for CUDA backend, you need to set the correct paths for the OpenCL library and then you can run cmake with Then you can run cmake with \emph{make  -DSUPPORT\_OCL=ON -DSUPPORT\_CUDA=OFF ..}

\item OpenCL backend -- when compiling the library with OpenCL (with cmake or with Makefile), during the compilation process you will be asked to select an OpenCL platform and device (if you have more than one). By doing that, the library will select the optimal number of threads and blocks for your hardware. Later on, you can change the platform and device, via the \emph{set\_ocl\_paralution()} or \emph{select\_device\_paralution()} function, but the threads configuration will be not changed. 

\item MIC backend -- the Intel Compiler should be loaded (\emph{icc}), then run the camke with \emph{cmake -DSUPPORT\_MIC=ON -DSUPPORT\_CUDA=OFF -DSUPPORT\_OCL=OFF  ..}

\end{itemize}



\section{Portability}

\begin{itemize}
\itemsep0em

\item Different backends -- you do not have to recompile your code if you want to run your program with different backends. You just need to load the corresponding dynamic library. As an example, if you compile the library with OpenCL support in \emph{/usr/local/paralution-ocl/build/lib} and with CUDA support in \emph{/usr/local/paralution-cuda/build/lib}, you will just need to set the path (i.e. \emph{export LD\_LIBRARY\_PATH= \$LD\_LIBRARY\_PATH:/usr/local/paralution-ocl/build/lib} or \emph{export LD\_LIBRARY\_PATH= \$LD\_LIBRARY\_PATH: /usr/local/paralution-cuda/build/lib}) and just run your program.

\end{itemize}
